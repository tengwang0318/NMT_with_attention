{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "NMT_with_word_tokenizer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciLuPwgSQ6nr"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "import jieba"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwPYbhJhQ9SC",
        "outputId": "caa3b306-a650-4a21-c8f4-c440fc562638"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9jht4peRf_u",
        "outputId": "06b62214-0cee-42d3-c2cd-3dfb9ad1913e"
      },
      "source": [
        "import os\n",
        "print(os.listdir(\"/content/drive/MyDrive\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['news-commentary-v13.zh-en.zh', 'news-commentary-v13.zh-en.en', 'Colab Notebooks', 'sentimental-analysis', 'cmn.txt', 'cmn-eng.zip', 'ckpt-4.data-00000-of-00001']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7pEOo4CRtxh"
      },
      "source": [
        "path_to_file = \"/content/drive/MyDrive/cmn.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsELGePaSQHF"
      },
      "source": [
        "# Converts the unicode file to ascii\n",
        "def unicode_to_ascii(s):\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "                 if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "#   w = unicode_to_ascii(w.lower().strip())\n",
        "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "  w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "  w = re.sub(r\"[^a-zA-Z\\u4e00-\\u9fa5?.!,¿]+\", \" \", w)\n",
        "  w = w.strip()\n",
        "\n",
        "  w = '<start> ' + w + ' <end>'\n",
        "  return w\n",
        "def zh_preprocess_sentence(w):\n",
        "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "    w = re.sub(r\"[^a-zA-Z\\u4e00-\\u9fa5?.!,¿]+\", \" \", w)\n",
        "    w = w.strip()\n",
        "    w = \" \".join(jieba.cut(w))\n",
        "    w = '<start> ' + w + \" <end>\"\n",
        "    return w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwBddTieSQXM"
      },
      "source": [
        "def create_dataset(path, num_examples):\n",
        "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "  \n",
        "#   word_pairs = [[preprocess_sentence(w) for w in line.split('\\t')[:2]]\n",
        "#                 for line in lines[:num_examples]]\n",
        "  word_pairs = [[preprocess_sentence(line.split(\"\\t\")[0]), zh_preprocess_sentence(line.split(\"\\t\")[1])] for line in lines]\n",
        "  return zip(*word_pairs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84u0pqWYSdw4",
        "outputId": "ce22514c-2540-4ae9-86f5-d2b99f34254e"
      },
      "source": [
        "en, zh = create_dataset(path_to_file,None)\n",
        "print(en[-1])\n",
        "print(zh[-1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 1.076 seconds.\n",
            "Prefix dict has been built successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<start> If a person has not had a chance to acquire his target language by the time he s an adult , he s unlikely to be able to reach native speaker level in that language . <end>\n",
            "<start> 如果 一個 人 在 成人 前 沒 有 機會習 得 目標 語言   他 對 該 語言 的 認識 達 到 母語者 程度 的 機會 是 相當 小 的 <end>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkwbWHerTJdL"
      },
      "source": [
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                         padding='post')\n",
        "\n",
        "  return tensor, lang_tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "al8B6Cs_USOD"
      },
      "source": [
        "\n",
        "def load_dataset(path, num_examples=None):\n",
        "  # creating cleaned input, output pairs\n",
        "  targ_lang, inp_lang = create_dataset(path, num_examples)\n",
        "\n",
        "  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "\n",
        "  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvFdMeSwUVo8"
      },
      "source": [
        "\n",
        "# Try experimenting with the size of that dataset\n",
        "# num_examples = 30000\n",
        "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file,\n",
        "                                                                num_examples)\n",
        "\n",
        "# Calculate max_length of the target tensors\n",
        "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bg0de2MUbU3",
        "outputId": "b907804f-8eab-40a5-f666-6cba08a46171"
      },
      "source": [
        "\n",
        "# Creating training and validation sets using an 80-20 split\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "\n",
        "# Show length\n",
        "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21110 21110 5278 5278\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FE1Cl9ZoUfvX"
      },
      "source": [
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t != 0:\n",
        "      print(f'{t} ----> {lang.index_word[t]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWAru4VOUiiV",
        "outputId": "a78b9df8-d003-4f30-b700-aa402ebca37f"
      },
      "source": [
        "print(\"Input Language; index to word mapping\")\n",
        "convert(inp_lang, input_tensor_train[0])\n",
        "print()\n",
        "print(\"Target Language; index to word mapping\")\n",
        "convert(targ_lang, target_tensor_train[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Language; index to word mapping\n",
            "1 ----> <start>\n",
            "2083 ----> 不幸\n",
            "4 ----> 的\n",
            "9 ----> 是\n",
            "10 ----> 她\n",
            "12 ----> 不\n",
            "8 ----> 在\n",
            "2 ----> <end>\n",
            "\n",
            "Target Language; index to word mapping\n",
            "1 ----> <start>\n",
            "2137 ----> unfortunately\n",
            "23 ----> ,\n",
            "22 ----> she\n",
            "10 ----> is\n",
            "799 ----> absent\n",
            "3 ----> .\n",
            "2 ----> <end>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2CJzT8TUkYI"
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vp5M-fPBVJj-",
        "outputId": "96b5582a-c616-409c-f01b-018a365e78d2"
      },
      "source": [
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 30]), TensorShape([64, 38]))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LDhOhcOVeq9"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state=hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93BBThgzV1sc",
        "outputId": "07cb7d88-f925-4871-82ab-7738f2f89ff2"
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "print('Encoder output shape: (batch size, sequence length, units)', sample_output.shape)\n",
        "print('Encoder Hidden state shape: (batch size, units)', sample_hidden.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (64, 30, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxi36lfhWZNc"
      },
      "source": [
        "\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    # query hidden state shape == (batch_size, hidden size)\n",
        "    # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # values shape == (batch_size, max_len, hidden size)\n",
        "    # we are doing this to broadcast addition along the time axis to calculate the score\n",
        "    query_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sX8wBYfOZFdK",
        "outputId": "40155a29-c04f-421c-f6c0-7c3d07b7b3d2"
      },
      "source": [
        "attention_layer = BahdanauAttention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units)\", attention_result.shape)\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1)\", attention_weights.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention result shape: (batch size, units) (64, 1024)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (64, 30, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nMG43daZIaP"
      },
      "source": [
        "\n",
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # used for attention\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACtzIKTUZKk9",
        "outputId": "97bf4f4e-a511-4aa7-dcf9-e072efc6f48a"
      },
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "\n",
        "print('Decoder output shape: (batch_size, vocab size)', sample_decoder_output.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (64, 6756)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0ZY79TUZMc6"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
        "                                                            reduction='none')\n",
        "\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vh0eu81tZOp8"
      },
      "source": [
        "\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRTwP8wcZQh1"
      },
      "source": [
        "\n",
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kLq_yn9ZSy_",
        "outputId": "41e1aa4a-4811-4d8c-a9c2-cee840f46ed8"
      },
      "source": [
        "EPOCHS = 20\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print(f'Epoch {epoch+1} Batch {batch} Loss {batch_loss.numpy():.4f}')\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix=checkpoint_prefix)\n",
        "\n",
        "  print(f'Epoch {epoch+1} Loss {total_loss/steps_per_epoch:.4f}')\n",
        "  print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 2.0667\n",
            "Epoch 1 Batch 100 Loss 1.1113\n",
            "Epoch 1 Batch 200 Loss 1.0104\n",
            "Epoch 1 Batch 300 Loss 0.9038\n",
            "Epoch 1 Loss 1.0367\n",
            "Time taken for 1 epoch 290.21 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.8173\n",
            "Epoch 2 Batch 100 Loss 0.7970\n",
            "Epoch 2 Batch 200 Loss 0.8626\n",
            "Epoch 2 Batch 300 Loss 0.7334\n",
            "Epoch 2 Loss 0.7977\n",
            "Time taken for 1 epoch 233.66 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.7129\n",
            "Epoch 3 Batch 100 Loss 0.6850\n",
            "Epoch 3 Batch 200 Loss 0.6505\n",
            "Epoch 3 Batch 300 Loss 0.6440\n",
            "Epoch 3 Loss 0.6622\n",
            "Time taken for 1 epoch 231.19 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.5847\n",
            "Epoch 4 Batch 100 Loss 0.5294\n",
            "Epoch 4 Batch 200 Loss 0.5904\n",
            "Epoch 4 Batch 300 Loss 0.5457\n",
            "Epoch 4 Loss 0.5377\n",
            "Time taken for 1 epoch 231.84 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.4158\n",
            "Epoch 5 Batch 100 Loss 0.4445\n",
            "Epoch 5 Batch 200 Loss 0.3991\n",
            "Epoch 5 Batch 300 Loss 0.4630\n",
            "Epoch 5 Loss 0.4238\n",
            "Time taken for 1 epoch 231.40 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.3768\n",
            "Epoch 6 Batch 100 Loss 0.3441\n",
            "Epoch 6 Batch 200 Loss 0.3685\n",
            "Epoch 6 Batch 300 Loss 0.3584\n",
            "Epoch 6 Loss 0.3254\n",
            "Time taken for 1 epoch 231.94 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.2512\n",
            "Epoch 7 Batch 100 Loss 0.2277\n",
            "Epoch 7 Batch 200 Loss 0.2277\n",
            "Epoch 7 Batch 300 Loss 0.2549\n",
            "Epoch 7 Loss 0.2457\n",
            "Time taken for 1 epoch 231.03 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.1634\n",
            "Epoch 8 Batch 100 Loss 0.1818\n",
            "Epoch 8 Batch 200 Loss 0.2098\n",
            "Epoch 8 Batch 300 Loss 0.1614\n",
            "Epoch 8 Loss 0.1843\n",
            "Time taken for 1 epoch 233.33 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.1693\n",
            "Epoch 9 Batch 100 Loss 0.1300\n",
            "Epoch 9 Batch 200 Loss 0.1522\n",
            "Epoch 9 Batch 300 Loss 0.1920\n",
            "Epoch 9 Loss 0.1379\n",
            "Time taken for 1 epoch 232.44 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.0803\n",
            "Epoch 10 Batch 100 Loss 0.1233\n",
            "Epoch 10 Batch 200 Loss 0.0826\n",
            "Epoch 10 Batch 300 Loss 0.1040\n",
            "Epoch 10 Loss 0.1037\n",
            "Time taken for 1 epoch 232.92 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 0.0910\n",
            "Epoch 11 Batch 100 Loss 0.1064\n",
            "Epoch 11 Batch 200 Loss 0.0780\n",
            "Epoch 11 Batch 300 Loss 0.0703\n",
            "Epoch 11 Loss 0.0780\n",
            "Time taken for 1 epoch 233.34 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 0.0601\n",
            "Epoch 12 Batch 100 Loss 0.0665\n",
            "Epoch 12 Batch 200 Loss 0.0658\n",
            "Epoch 12 Batch 300 Loss 0.0635\n",
            "Epoch 12 Loss 0.0602\n",
            "Time taken for 1 epoch 233.74 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.0426\n",
            "Epoch 13 Batch 100 Loss 0.0449\n",
            "Epoch 13 Batch 200 Loss 0.0459\n",
            "Epoch 13 Batch 300 Loss 0.0440\n",
            "Epoch 13 Loss 0.0482\n",
            "Time taken for 1 epoch 233.40 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 0.0383\n",
            "Epoch 14 Batch 100 Loss 0.0348\n",
            "Epoch 14 Batch 200 Loss 0.0414\n",
            "Epoch 14 Batch 300 Loss 0.0488\n",
            "Epoch 14 Loss 0.0402\n",
            "Time taken for 1 epoch 233.84 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.0254\n",
            "Epoch 15 Batch 100 Loss 0.0331\n",
            "Epoch 15 Batch 200 Loss 0.0272\n",
            "Epoch 15 Batch 300 Loss 0.0421\n",
            "Epoch 15 Loss 0.0358\n",
            "Time taken for 1 epoch 232.77 sec\n",
            "\n",
            "Epoch 16 Batch 0 Loss 0.0222\n",
            "Epoch 16 Batch 100 Loss 0.0519\n",
            "Epoch 16 Batch 200 Loss 0.0289\n",
            "Epoch 16 Batch 300 Loss 0.0399\n",
            "Epoch 16 Loss 0.0323\n",
            "Time taken for 1 epoch 234.04 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.0291\n",
            "Epoch 17 Batch 100 Loss 0.0411\n",
            "Epoch 17 Batch 200 Loss 0.0300\n",
            "Epoch 17 Batch 300 Loss 0.0266\n",
            "Epoch 17 Loss 0.0292\n",
            "Time taken for 1 epoch 232.91 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.0236\n",
            "Epoch 18 Batch 100 Loss 0.0182\n",
            "Epoch 18 Batch 200 Loss 0.0301\n",
            "Epoch 18 Batch 300 Loss 0.0253\n",
            "Epoch 18 Loss 0.0270\n",
            "Time taken for 1 epoch 234.20 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.0289\n",
            "Epoch 19 Batch 100 Loss 0.0205\n",
            "Epoch 19 Batch 200 Loss 0.0393\n",
            "Epoch 19 Batch 300 Loss 0.0354\n",
            "Epoch 19 Loss 0.0264\n",
            "Time taken for 1 epoch 233.32 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.0144\n",
            "Epoch 20 Batch 100 Loss 0.0258\n",
            "Epoch 20 Batch 200 Loss 0.0285\n",
            "Epoch 20 Batch 300 Loss 0.0381\n",
            "Epoch 20 Loss 0.0256\n",
            "Time taken for 1 epoch 233.77 sec\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vW6-OARlZVYH"
      },
      "source": [
        "\n",
        "def evaluate(sentence):\n",
        "  attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "\n",
        "  sentence = preprocess_sentence(sentence)\n",
        "\n",
        "  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_length_inp,\n",
        "                                                         padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "  result = ''\n",
        "\n",
        "  hidden = [tf.zeros((1, units))]\n",
        "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
        "\n",
        "  for t in range(max_length_targ):\n",
        "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                         dec_hidden,\n",
        "                                                         enc_out)\n",
        "\n",
        "    # storing the attention weights to plot later on\n",
        "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "    attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "    result += targ_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "    if targ_lang.index_word[predicted_id] == '<end>':\n",
        "      return result, sentence, attention_plot\n",
        "\n",
        "    # the predicted ID is fed back into the model\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  return result, sentence, attention_plot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DO1ZmKbbZYM4"
      },
      "source": [
        "\n",
        "# function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "  fig = plt.figure(figsize=(10, 10))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "  ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "  fontdict = {'fontsize': 14}\n",
        "\n",
        "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feQutp2_ZaNJ"
      },
      "source": [
        "def translate(sentence):\n",
        "  sentence = \" \".join(jieba.cut(sentence))\n",
        "  result, sentence, attention_plot = evaluate(sentence)\n",
        "\n",
        "  print('Input:', sentence)\n",
        "  print('Predicted translation:', result)\n",
        "\n",
        "#   attention_plot = attention_plot[:len(result.split(' ')),\n",
        "#                                   :len(sentence.split(' '))]\n",
        "#   plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhpnEETyZcBv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3d7bef6-dc33-483d-dd0b-dfde511da2f6"
      },
      "source": [
        "translate(\"我不想再做这个任务\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: <start> 我 不想 再 做 这个 任务 <end>\n",
            "Predicted translation: i don t want to do this again . <end> \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAV8jtn3kYrT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a640e134-d243-4675-ef84-2038d5681def"
      },
      "source": [
        "translate(\"困死了,我想睡觉了\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: <start> 困死 了 , 我 想 睡觉 了 <end>\n",
            "Predicted translation: i m almost , i wanted to sleep . <end> \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENyW7J6xlGMl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e98d4eb3-c8bc-47ae-a3e4-0ab070a58f8e"
      },
      "source": [
        "translate(\"我喜欢这幅画，不仅是因为它的名气，而是因为它真的是一个杰作。\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: <start> 我 喜欢 这幅 画 不仅 是因为 它 的 名气 而是 因为 它 真的 是 一个 杰作 <end>\n",
            "Predicted translation: i like this picture , not just because it really not a masterpiece . <end> \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HE0L5rnmEcb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d6cf7db-2d2f-4898-e7f9-b28080b13215"
      },
      "source": [
        "translate(\"假如你在老师讲课的时候再集中一点去听讲的话，你应该就能弄明白了。\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: <start> 假如 你 在 老师 讲课 的 时候 再 集中 一点 去 听讲 的话 你 应该 就 能 弄 明白 了 <end>\n",
            "Predicted translation: if you d listen a little more carefully to what the teacher says , you d probably be able to what the teacher says , you d probably be able to what the teacher says , you d \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAtuJkonmJdp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07e562c4-488b-4be7-c6b3-b5ff6faabc71"
      },
      "source": [
        "translate(\"我不想学习，我只想出去玩\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: <start> 我 不想 学习 我 只 想 出去玩 <end>\n",
            "Predicted translation: i didn t want to study to go to play . <end> \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flcOnwCimS--",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73009366-d648-4abb-acae-94e8219d58fc"
      },
      "source": [
        "translate(\"英语对于我来说太难了\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: <start> 英语 对于 我 来说 太难 了 <end>\n",
            "Predicted translation: english is too difficult for me to do . <end> \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BonprmacWhbt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f421546-ed61-41a9-d673-253109639b7f"
      },
      "source": [
        "translate(\"汤姆去河里游泳，但当他出来时，他的衣服被偷了。\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: <start> 汤姆 去 河里 游泳 但 当 他 出来 时 他 的 衣服 被 偷 了 <end>\n",
            "Predicted translation: tom went swimming in the river , his clothes had been stolen . <end> \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRL1ENe8mDI_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}