{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NMT_with_vc13.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fn1_d5NbO_q9"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "import jieba"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_s___USOPjdT",
        "outputId": "0b5dd800-c027-4bb0-e1cc-85f1265fd14c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JshyFCcqP4ra",
        "outputId": "f26eeb3e-6f0e-4eeb-b958-71e270f5f36a"
      },
      "source": [
        "import os\n",
        "print(os.listdir(\"/content/drive/MyDrive\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['news-commentary-v13.zh-en.zh', 'news-commentary-v13.zh-en.en', 'Colab Notebooks', 'sentimental-analysis', 'cmn.txt', 'cmn-eng.zip', 'ckpt-4.data-00000-of-00001']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkBB1AgMQIao"
      },
      "source": [
        "en_path_to_file = \"/content/drive/MyDrive/news-commentary-v13.zh-en.en\"\n",
        "zh_path_to_file = \"/content/drive/MyDrive/news-commentary-v13.zh-en.zh\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rz2ncRIiQhZq"
      },
      "source": [
        "# Converts the unicode file to ascii\n",
        "def unicode_to_ascii(s):\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "                 if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "#   w = unicode_to_ascii(w.lower().strip())\n",
        "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "  w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "  w = re.sub(r\"[^a-zA-Z\\u4e00-\\u9fa5?.!,¿]+\", \" \", w)\n",
        "  w = w.strip()\n",
        "\n",
        "  w = '<start> ' + w + ' <end>'\n",
        "  return w\n",
        "def zh_preprocess_sentence(w):\n",
        "    w = w[:-1]\n",
        "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "    w = re.sub(r\"[^0-9a-zA-Z\\u4e00-\\u9fa5?.!,¿，。？！；]+\", \" \", w)\n",
        "    w = w.strip()\n",
        "    w = \" \".join(jieba.cut(w))\n",
        "    w = '<start> ' + w + \" <end>\"\n",
        "    return w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HQqrkRDQoMu"
      },
      "source": [
        "def create_dataset(en_path, zh_path, num_examples):\n",
        "    lines = io.open(en_path, encoding='UTF8').readlines()\n",
        "    en = [preprocess_sentence(sentence) for sentence in lines[:num_examples]]\n",
        "    lines = io.open(zh_path, encoding='UTF8').readlines()\n",
        "    \n",
        "    zh = [zh_preprocess_sentence(sentence) for sentence in lines[:num_examples]]\n",
        "\n",
        "    return  en, zh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZadlfRJ3RdW-",
        "outputId": "0c16363d-cf0a-4948-bab7-92997636adde"
      },
      "source": [
        "en, zh = create_dataset(en_path_to_file, zh_path_to_file, None)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 0.784 seconds.\n",
            "Prefix dict has been built successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QAP_-fmRjjq",
        "outputId": "5b146c7b-1c8c-4a39-e2f9-2f644fef87f2"
      },
      "source": [
        "w = \"2020年过去了，2021年来了\"\n",
        "w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "# replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "w = re.sub(r\"[^0-9a-zA-Z\\u4e00-\\u9fa5?.!,¿，。？！；]+\", \" \", w)\n",
        "print(w)\n",
        "w = w.strip()\n",
        "w = \" \".join(jieba.cut(w))\n",
        "print(w)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2020年过去了，2021年来了\n",
            "2020 年 过去 了 ， 2021 年 来 了\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5c3a1IXRqqj",
        "outputId": "efc0593c-03a5-418e-9fb8-e8ffc0b49c0b"
      },
      "source": [
        "zh[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<start> 1929 年 还是 1989 年   ? <end>',\n",
              " '<start> 巴黎   随着 经济危机 不断 加深 和 蔓延 ， 整个 世界 一直 在 寻找 历史 上 的 类似 事件 希望 有助于 我们 了解 目前 正在 发生 的 情况 。 <end>',\n",
              " '<start> 一 开始 ， 很多 人 把 这次 危机 比作 1982 年 或 1973 年 所 发生 的 情况 ， 这样 得 类比 是 令人 宽心 的 ， 因为 这 两段 时期 意味着 典型 的 周期性 衰退 。 <end>',\n",
              " '<start> 如今 人们 的 心情 却是 沉重 多 了 ， 许多 人 开始 把 这次 危机 与 1929 年 和 1931 年 相比 ， 即使 一些 国家 政府 的 表现 仍然 似乎 把视 目前 的 情况 为 是 典型 的 而 看见 的 衰退 。 <end>',\n",
              " '<start> 目前 的 趋势 是 ， 要么 是 过度 的 克制   欧洲   ， 要么 是 努力 的 扩展   美国   。 <end>',\n",
              " '<start> 欧洲 在 避免 债务 和 捍卫 欧元 的 名义 下正 变得 谨慎 ， 而 美国 已经 在 许多 方面 行动 起来 ， 以 利用 这一 理想 的 时机 来 实行 急需 的 结构性 改革 。 <end>',\n",
              " '<start> 然而 ， 作为 地域 战略 学家 ， 无论是 从 政治 意义 还是 从 经济 意义 上 ， 让 我 自然 想到 的 年份 是 1989 年 。 <end>',\n",
              " '<start> 当然 ， 雷曼 兄弟 公司 的 倒闭 和 柏林墙 的 倒塌 没有 任何 关系 。 <end>',\n",
              " '<start> 事实上 ， 从 表面 上 看 ， 两者 似乎 是 完全 是 相反 的   一个 是 象征 着 压抑 和 人为 分裂 的 柏林墙 的 倒塌 ， 而 另 一个 是 看似 坚不可摧 的 并 令人 安心 的 金融 资本主义 机构 的 倒塌 。 <end>',\n",
              " '<start> 然而 ， 和 1989 年 一样 ， 2008   2009 年 很 可能 也 能 被 视为 一个 划时代 的 改变 ， 其 带来 的 发人深省 的 后果 将 在 几十年 后 仍 能 让 我们 感受 得到 。 <end>']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dXNU79NSVkW"
      },
      "source": [
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                         padding='post')\n",
        "\n",
        "  return tensor, lang_tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eW-bi1y_Txj8"
      },
      "source": [
        "\n",
        "def load_dataset(en_path_to_file, zh_path_to_file, num_examples=None):\n",
        "  # creating cleaned input, output pairs\n",
        "  targ_lang, inp_lang = create_dataset(en_path_to_file, zh_path_to_file, num_examples)\n",
        "\n",
        "  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "\n",
        "  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZ6-mmFKUyq0"
      },
      "source": [
        "\n",
        "# Try experimenting with the size of that dataset\n",
        "num_examples = 30000\n",
        "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(en_path_to_file, zh_path_to_file, num_examples)\n",
        "\n",
        "# Calculate max_length of the target tensors\n",
        "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZm9bkBPVVxr",
        "outputId": "1ddc27f4-14c5-4dd8-9516-156fd37cc525"
      },
      "source": [
        "\n",
        "# Creating training and validation sets using an 80-20 split\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "\n",
        "# Show length\n",
        "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24000 24000 6000 6000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbPfNc1gVYL1"
      },
      "source": [
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t != 0:\n",
        "      print(f'{t} ----> {lang.index_word[t]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6MmdTn2VZ6n",
        "outputId": "9894520e-2fe0-44c4-cf57-b48f65ac8bd3"
      },
      "source": [
        "print(\"Input Language; index to word mapping\")\n",
        "convert(inp_lang, input_tensor_train[0])\n",
        "print()\n",
        "print(\"Target Language; index to word mapping\")\n",
        "convert(targ_lang, target_tensor_train[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Language; index to word mapping\n",
            "3 ----> <start>\n",
            "6 ----> 在\n",
            "13 ----> 对\n",
            "2791 ----> 纳米技术\n",
            "302 ----> 领导\n",
            "418 ----> 地位\n",
            "1 ----> 的\n",
            "48 ----> 全球\n",
            "619 ----> 竞争\n",
            "22 ----> 中\n",
            "2 ----> ，\n",
            "190 ----> 只有\n",
            "154 ----> 那些\n",
            "13 ----> 对\n",
            "40 ----> 其\n",
            "4483 ----> 危险性\n",
            "24 ----> 有\n",
            "135 ----> 很\n",
            "402 ----> 好\n",
            "1 ----> 的\n",
            "604 ----> 认识\n",
            "214 ----> 并且\n",
            "86 ----> 支持\n",
            "721 ----> 必要\n",
            "1 ----> 的\n",
            "221 ----> 研究\n",
            "69 ----> 以\n",
            "125 ----> 把\n",
            "131 ----> 这种\n",
            "4483 ----> 危险性\n",
            "3672 ----> 降到\n",
            "2203 ----> 最低\n",
            "391 ----> 程度\n",
            "1 ----> 的\n",
            "16 ----> 国家\n",
            "414 ----> 才\n",
            "29 ----> 会\n",
            "100 ----> 成为\n",
            "370 ----> 最后\n",
            "1 ----> 的\n",
            "2204 ----> 赢家\n",
            "5 ----> 。\n",
            "4 ----> <end>\n",
            "\n",
            "Target Language; index to word mapping\n",
            "3 ----> <start>\n",
            "9 ----> in\n",
            "1 ----> the\n",
            "52 ----> global\n",
            "1988 ----> race\n",
            "14 ----> for\n",
            "2914 ----> nanotechnology\n",
            "340 ----> leadership\n",
            "2 ----> ,\n",
            "1 ----> the\n",
            "3008 ----> winners\n",
            "26 ----> will\n",
            "16 ----> be\n",
            "115 ----> those\n",
            "72 ----> who\n",
            "839 ----> understand\n",
            "1 ----> the\n",
            "485 ----> risks\n",
            "8 ----> and\n",
            "152 ----> support\n",
            "1 ----> the\n",
            "445 ----> research\n",
            "612 ----> necessary\n",
            "7 ----> to\n",
            "3584 ----> minimize\n",
            "111 ----> them\n",
            "5 ----> .\n",
            "4 ----> <end>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWgKvUcqVb5g"
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 128\n",
        "units = 512\n",
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-s_2ACrVg1f"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state=hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXt6EdkKWQOo"
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5f1Q4IEWNWl"
      },
      "source": [
        "\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    # query hidden state shape == (batch_size, hidden size)\n",
        "    # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # values shape == (batch_size, max_len, hidden size)\n",
        "    # we are doing this to broadcast addition along the time axis to calculate the score\n",
        "    query_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HAbLH4iWWXB"
      },
      "source": [
        "\n",
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # used for attention\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Z5XQc3WWZUi"
      },
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rS8AiVaNWbac"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
        "                                                            reduction='none')\n",
        "\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mthgzalcWc7T"
      },
      "source": [
        "\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCHL88kvWehd"
      },
      "source": [
        "\n",
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2B6y6-rPWhRU",
        "outputId": "dc86c604-293c-4e69-b71e-542fbfc83244"
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print(f'Epoch {epoch+1} Batch {batch} Loss {batch_loss.numpy():.4f}')\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix=checkpoint_prefix)\n",
        "\n",
        "  print(f'Epoch {epoch+1} Loss {total_loss/steps_per_epoch:.4f}')\n",
        "  print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 2.4356\n",
            "Epoch 1 Batch 100 Loss 1.5084\n",
            "Epoch 1 Batch 200 Loss 1.5473\n",
            "Epoch 1 Batch 300 Loss 1.5258\n",
            "Epoch 1 Loss 1.5894\n",
            "Time taken for 1 epoch 415.07 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.2561\n",
            "Epoch 2 Batch 100 Loss 1.5227\n",
            "Epoch 2 Batch 200 Loss 1.5879\n",
            "Epoch 2 Batch 300 Loss 1.2781\n",
            "Epoch 2 Loss 1.4534\n",
            "Time taken for 1 epoch 309.08 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.3145\n",
            "Epoch 3 Batch 100 Loss 1.4520\n",
            "Epoch 3 Batch 200 Loss 1.4190\n",
            "Epoch 3 Batch 300 Loss 1.3531\n",
            "Epoch 3 Loss 1.3815\n",
            "Time taken for 1 epoch 309.30 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.2197\n",
            "Epoch 4 Batch 100 Loss 1.3635\n",
            "Epoch 4 Batch 200 Loss 1.1740\n",
            "Epoch 4 Batch 300 Loss 1.3564\n",
            "Epoch 4 Loss 1.3174\n",
            "Time taken for 1 epoch 309.35 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 1.1783\n",
            "Epoch 5 Batch 100 Loss 1.2386\n",
            "Epoch 5 Batch 200 Loss 1.2464\n",
            "Epoch 5 Batch 300 Loss 1.2403\n",
            "Epoch 5 Loss 1.2565\n",
            "Time taken for 1 epoch 308.88 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 1.3047\n",
            "Epoch 6 Batch 100 Loss 1.2343\n",
            "Epoch 6 Batch 200 Loss 1.2250\n",
            "Epoch 6 Batch 300 Loss 1.0828\n",
            "Epoch 6 Loss 1.1986\n",
            "Time taken for 1 epoch 309.03 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 1.0138\n",
            "Epoch 7 Batch 100 Loss 1.1322\n",
            "Epoch 7 Batch 200 Loss 1.1930\n",
            "Epoch 7 Batch 300 Loss 1.2477\n",
            "Epoch 7 Loss 1.1435\n",
            "Time taken for 1 epoch 308.34 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 1.1530\n",
            "Epoch 8 Batch 100 Loss 0.9573\n",
            "Epoch 8 Batch 200 Loss 1.0059\n",
            "Epoch 8 Batch 300 Loss 1.1343\n",
            "Epoch 8 Loss 1.0924\n",
            "Time taken for 1 epoch 308.53 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 1.1089\n",
            "Epoch 9 Batch 100 Loss 0.9485\n",
            "Epoch 9 Batch 200 Loss 1.0907\n",
            "Epoch 9 Batch 300 Loss 1.0346\n",
            "Epoch 9 Loss 1.0433\n",
            "Time taken for 1 epoch 308.20 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.9294\n",
            "Epoch 10 Batch 100 Loss 0.9786\n",
            "Epoch 10 Batch 200 Loss 1.0177\n",
            "Epoch 10 Batch 300 Loss 1.0232\n",
            "Epoch 10 Loss 0.9944\n",
            "Time taken for 1 epoch 308.46 sec\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YL0IGzQxWi4m"
      },
      "source": [
        "\n",
        "def evaluate(sentence):\n",
        "  attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "\n",
        "  sentence = preprocess_sentence(sentence)\n",
        "\n",
        "  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_length_inp,\n",
        "                                                         padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "  result = ''\n",
        "\n",
        "  hidden = [tf.zeros((1, units))]\n",
        "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
        "\n",
        "  for t in range(max_length_targ):\n",
        "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                         dec_hidden,\n",
        "                                                         enc_out)\n",
        "\n",
        "    # storing the attention weights to plot later on\n",
        "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "    attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "    result += targ_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "    if targ_lang.index_word[predicted_id] == '<end>':\n",
        "      return result, sentence, attention_plot\n",
        "\n",
        "    # the predicted ID is fed back into the model\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  return result, sentence, attention_plot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8eWlTzUjWkcL"
      },
      "source": [
        "\n",
        "# function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "  plt.rcParams['font.family'] = ['sans-serif'] \n",
        "  fig = plt.figure(figsize=(10, 10))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "  ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "  fontdict = {'fontsize': 14}\n",
        "\n",
        "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFpfdIjxWm3s"
      },
      "source": [
        "def translate(sentence):\n",
        "  sentence = \" \".join(jieba.cut(sentence))\n",
        "  result, sentence, attention_plot = evaluate(sentence)\n",
        "\n",
        "  print('Input:', sentence)\n",
        "  print('Predicted translation:', result)\n",
        "\n",
        "#   attention_plot = attention_plot[:len(result.split(' ')),\n",
        "#                                   :len(sentence.split(' '))]\n",
        "#   plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m89T4L3rWoI-",
        "outputId": "0b7434af-56b1-4033-bc99-6f8aadbe404a"
      },
      "source": [
        "translate(\"你怎么了\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: <start> 你 怎么 了 <end>\n",
            "Predicted translation: you know how to be done . <end> \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSlFmBtlkMTX",
        "outputId": "ebbf16fe-cbd8-4b8d-d34c-801bb4d5a109"
      },
      "source": [
        "translate(\"为什么训练结果很不好？\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: <start> 为什么 训练 结果 很 不好 <end>\n",
            "Predicted translation: why , there is no one . <end> \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Ja5eCFfkaz_",
        "outputId": "b3bf2dfd-6c06-4fb3-e04d-26b43887777b"
      },
      "source": [
        "translate(\"那我就继续训练\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: <start> 那 我 就 继续 训练 <end>\n",
            "Predicted translation: what i am not just as a few of the same . <end> \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BH2KptrDkeKI",
        "outputId": "878cbd28-aac8-442b-9cc7-55a7e72a7289"
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(11,11 + EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print(f'Epoch {epoch+1} Batch {batch} Loss {batch_loss.numpy():.4f}')\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix=checkpoint_prefix)\n",
        "\n",
        "  print(f'Epoch {epoch+1} Loss {total_loss/steps_per_epoch:.4f}')\n",
        "  print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12 Batch 0 Loss 0.9867\n",
            "Epoch 12 Batch 100 Loss 0.8365\n",
            "Epoch 12 Batch 200 Loss 0.9500\n",
            "Epoch 12 Batch 300 Loss 0.9165\n",
            "Epoch 12 Loss 0.9478\n",
            "Time taken for 1 epoch 309.13 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.8487\n",
            "Epoch 13 Batch 100 Loss 0.8797\n",
            "Epoch 13 Batch 200 Loss 0.9389\n",
            "Epoch 13 Batch 300 Loss 0.9228\n",
            "Epoch 13 Loss 0.9033\n",
            "Time taken for 1 epoch 308.18 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 0.8366\n",
            "Epoch 14 Batch 100 Loss 0.7698\n",
            "Epoch 14 Batch 200 Loss 0.7555\n",
            "Epoch 14 Batch 300 Loss 0.7917\n",
            "Epoch 14 Loss 0.8598\n",
            "Time taken for 1 epoch 308.85 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.8253\n",
            "Epoch 15 Batch 100 Loss 0.7587\n",
            "Epoch 15 Batch 200 Loss 0.8403\n",
            "Epoch 15 Batch 300 Loss 0.8884\n",
            "Epoch 15 Loss 0.8185\n",
            "Time taken for 1 epoch 308.46 sec\n",
            "\n",
            "Epoch 16 Batch 0 Loss 0.8029\n",
            "Epoch 16 Batch 100 Loss 0.7919\n",
            "Epoch 16 Batch 200 Loss 0.7926\n",
            "Epoch 16 Batch 300 Loss 0.7850\n",
            "Epoch 16 Loss 0.7797\n",
            "Time taken for 1 epoch 308.74 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.7042\n",
            "Epoch 17 Batch 100 Loss 0.7054\n",
            "Epoch 17 Batch 200 Loss 0.6306\n",
            "Epoch 17 Batch 300 Loss 0.7309\n",
            "Epoch 17 Loss 0.7428\n",
            "Time taken for 1 epoch 308.03 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.6907\n",
            "Epoch 18 Batch 100 Loss 0.7216\n",
            "Epoch 18 Batch 200 Loss 0.6615\n",
            "Epoch 18 Batch 300 Loss 0.6990\n",
            "Epoch 18 Loss 0.7076\n",
            "Time taken for 1 epoch 308.14 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.6417\n",
            "Epoch 19 Batch 100 Loss 0.6378\n",
            "Epoch 19 Batch 200 Loss 0.6929\n",
            "Epoch 19 Batch 300 Loss 0.6668\n",
            "Epoch 19 Loss 0.6750\n",
            "Time taken for 1 epoch 307.78 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.5580\n",
            "Epoch 20 Batch 100 Loss 0.6431\n",
            "Epoch 20 Batch 200 Loss 0.5786\n",
            "Epoch 20 Batch 300 Loss 0.6587\n",
            "Epoch 20 Loss 0.6450\n",
            "Time taken for 1 epoch 308.10 sec\n",
            "\n",
            "Epoch 21 Batch 0 Loss 0.5195\n",
            "Epoch 21 Batch 100 Loss 0.6227\n",
            "Epoch 21 Batch 200 Loss 0.5540\n",
            "Epoch 21 Batch 300 Loss 0.7363\n",
            "Epoch 21 Loss 0.6134\n",
            "Time taken for 1 epoch 307.88 sec\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLyMd2-Lkl-N",
        "outputId": "fa7b7349-455c-4062-d589-84889e015265"
      },
      "source": [
        "translate(\"这次的结果如何？\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: <start> 这次 的 结果 如何 <end>\n",
            "Predicted translation: the outcome ? <end> \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JT13D8NbwjEx",
        "outputId": "8f86e7a7-b5dd-4567-a3fb-3849426b0148"
      },
      "source": [
        "translate(\"感觉比上次好\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: <start> 感觉 比 上次 好 <end>\n",
            "Predicted translation: the more likely fall . <end> \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWS9tgJlwnbI",
        "outputId": "ab3e5d62-347a-4074-8527-a65ce89a9992"
      },
      "source": [
        "translate(\"看来训练的模型还是存在一些问题\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: <start> 看来 训练 的 模型 还是 存在 一些 问题 <end>\n",
            "Predicted translation: there is to be done . <end> \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_wVNg3zwsh9",
        "outputId": "c59b1596-5eed-40e8-b38d-e4b922103f91"
      },
      "source": [
        "translate(\"继续去运行代码\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: <start> 继续 去 运行 代码 <end>\n",
            "Predicted translation: no longer term . <end> \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ueNXlM3wxcd",
        "outputId": "7ec99be4-35d7-4aff-899b-5d59a69ee33c"
      },
      "source": [
        "translate(\"心情十分不好\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: <start> 心情 十分 不好 <end>\n",
            "Predicted translation: the second moment to be the case for it . <end> \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDB2ow4Gw0gC",
        "outputId": "bdfc7b0b-8bd2-41e2-d5ed-5995deeba19f"
      },
      "source": [
        "translate(\"真的浪费时间\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: <start> 真的 浪费时间 <end>\n",
            "Predicted translation: what went to explain to dare <end> \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-fTwRPjw3hq",
        "outputId": "cef380f2-da1a-499b-e098-1658fcc11461"
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(21,21 + EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print(f'Epoch {epoch+1} Batch {batch} Loss {batch_loss.numpy():.4f}')\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix=checkpoint_prefix)\n",
        "\n",
        "  print(f'Epoch {epoch+1} Loss {total_loss/steps_per_epoch:.4f}')\n",
        "  print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22 Batch 0 Loss 0.5870\n",
            "Epoch 22 Batch 100 Loss 0.6153\n",
            "Epoch 22 Batch 200 Loss 0.5793\n",
            "Epoch 22 Batch 300 Loss 0.5155\n",
            "Epoch 22 Loss 0.5826\n",
            "Time taken for 1 epoch 308.80 sec\n",
            "\n",
            "Epoch 23 Batch 0 Loss 0.4795\n",
            "Epoch 23 Batch 100 Loss 0.5278\n",
            "Epoch 23 Batch 200 Loss 0.5953\n",
            "Epoch 23 Batch 300 Loss 0.5855\n",
            "Epoch 23 Loss 0.5568\n",
            "Time taken for 1 epoch 307.90 sec\n",
            "\n",
            "Epoch 24 Batch 0 Loss 0.4495\n",
            "Epoch 24 Batch 100 Loss 0.4986\n",
            "Epoch 24 Batch 200 Loss 0.4979\n",
            "Epoch 24 Batch 300 Loss 0.5073\n",
            "Epoch 24 Loss 0.5309\n",
            "Time taken for 1 epoch 308.09 sec\n",
            "\n",
            "Epoch 25 Batch 0 Loss 0.5136\n",
            "Epoch 25 Batch 100 Loss 0.5087\n",
            "Epoch 25 Batch 200 Loss 0.4461\n",
            "Epoch 25 Batch 300 Loss 0.4962\n",
            "Epoch 25 Loss 0.5055\n",
            "Time taken for 1 epoch 307.79 sec\n",
            "\n",
            "Epoch 26 Batch 0 Loss 0.4839\n",
            "Epoch 26 Batch 100 Loss 0.4623\n",
            "Epoch 26 Batch 200 Loss 0.4831\n",
            "Epoch 26 Batch 300 Loss 0.4841\n",
            "Epoch 26 Loss 0.4814\n",
            "Time taken for 1 epoch 308.12 sec\n",
            "\n",
            "Epoch 27 Batch 0 Loss 0.4428\n",
            "Epoch 27 Batch 100 Loss 0.4060\n",
            "Epoch 27 Batch 200 Loss 0.4663\n",
            "Epoch 27 Batch 300 Loss 0.5396\n",
            "Epoch 27 Loss 0.4633\n",
            "Time taken for 1 epoch 307.79 sec\n",
            "\n",
            "Epoch 28 Batch 0 Loss 0.4164\n",
            "Epoch 28 Batch 100 Loss 0.4186\n",
            "Epoch 28 Batch 200 Loss 0.4267\n",
            "Epoch 28 Batch 300 Loss 0.5145\n",
            "Epoch 28 Loss 0.4395\n",
            "Time taken for 1 epoch 307.96 sec\n",
            "\n",
            "Epoch 29 Batch 0 Loss 0.3814\n",
            "Epoch 29 Batch 100 Loss 0.3401\n",
            "Epoch 29 Batch 200 Loss 0.4692\n",
            "Epoch 29 Batch 300 Loss 0.4510\n",
            "Epoch 29 Loss 0.4147\n",
            "Time taken for 1 epoch 307.58 sec\n",
            "\n",
            "Epoch 30 Batch 0 Loss 0.4198\n",
            "Epoch 30 Batch 100 Loss 0.4180\n",
            "Epoch 30 Batch 200 Loss 0.3033\n",
            "Epoch 30 Batch 300 Loss 0.3412\n",
            "Epoch 30 Loss 0.3969\n",
            "Time taken for 1 epoch 307.77 sec\n",
            "\n",
            "Epoch 31 Batch 0 Loss 0.3873\n",
            "Epoch 31 Batch 100 Loss 0.3569\n",
            "Epoch 31 Batch 200 Loss 0.3218\n",
            "Epoch 31 Batch 300 Loss 0.4849\n",
            "Epoch 31 Loss 0.3775\n",
            "Time taken for 1 epoch 307.32 sec\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "an9p59rTw_ea",
        "outputId": "f311821b-67ec-4e28-862b-f88f87ed90c6"
      },
      "source": [
        "translate(\"巴黎-随着经济危机不断加深和蔓延，整个世界一直在寻找历史上的类似事件希望有助于我们了解目前正在发生的情况。\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: <start> 巴黎 随着 经济危机 不断 加深 和 蔓延 整个 世界 一直 在 寻找 历史 上 的 类似 事件 希望 有助于 我们 了解 目前 正在 发生 的 情况 <end>\n",
            "Predicted translation: paris as the global recession is now in the world today is changing . <end> \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fi7_A-BQ87N0",
        "outputId": "dd189382-8adc-4fa6-9f4a-48280eed1dde"
      },
      "source": [
        "translate(\"当然，现在的情况和1989年的情况明显不同了。\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: <start> 当然 现在 的 情况 和 年 的 情况 明显 不同 了 <end>\n",
            "Predicted translation: now , there is that story of the lessons of the lessons of the lessons of the lessons of the lessons of the lessons of the lessons of the lessons of the lessons of the lessons of the lessons of the lessons of the lessons of the lessons of the lessons of the lessons of the lessons of the lessons of the lessons of the lessons of the lessons of the lessons of the lessons of the lessons of the lessons of the lessons of the lessons of the lessons of the lessons of the lessons of the lessons of the lessons of the lessons of the \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_b-elLSk9HId",
        "outputId": "2d39b287-ca0b-4ba9-ed76-456e7470c535"
      },
      "source": [
        "translate(\"作为哈佛大学和麻省理工学院的访问教授，我能看到危机过后的世界是什么样子的。\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: <start> 作为 哈佛大学 和 麻省理工学院 的 访问 教授 我能 看到 危机 过后 的 世界 是 什么 样子 的 <end>\n",
            "Predicted translation: as a similar view could cure the world could not be the world crisis in the global crisis could not be the world crisis in the global crisis could not be the world crisis in the global crisis could not be the world crisis in the global crisis could not be the world crisis in the global crisis could not be the world crisis in the global crisis could not be the world crisis in the global crisis could not be the world crisis in the global crisis could not be the world crisis in the global crisis could not be the world crisis in the global \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdgmPKlU9K5j",
        "outputId": "8b89fe7c-7e69-4641-d5fb-84634b975206"
      },
      "source": [
        "translate(\"每个人似乎都是输家，即使有些国家比其它国家受到的影响更大。\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: <start> 每个 人 似乎 都 是 输家 即使 有些 国家 比 其它 国家 受到 的 影响 更大 <end>\n",
            "Predicted translation: everyone seems to be more likely to be more likely to be more likely to be more likely to be more likely to be more likely to be more likely to be more likely to be more likely to be more likely to be more likely to be more likely to be more likely to be more likely to be more likely to be more likely to be more likely to be more likely to be more likely to be more likely to be more likely to be more likely to be more likely to be more likely to be more likely to be more likely to \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wrz6HHUZ9Tuz",
        "outputId": "60f50386-ca02-4d7d-afc2-d6c1dafd51c5"
      },
      "source": [
        "translate(\"你实际上必须实施解决办法——并且在事实证明你知道的并不如你认为的那样多时，你得愿意改变办法。\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: <start> 你 实际上 必须 实施 解决办法 并且 在 事实证明 你 知道 的 并 不如 你 认为 的 那样 多时 你 得 愿意 改变 办法 <end>\n",
            "Predicted translation: you cannot afford to mind about the sidelines of disruption from the sidelines of disruption from the sidelines of disruption from the sidelines of disruption from the sidelines of disruption from the sidelines of disruption from the sidelines of disruption from the sidelines of disruption from the sidelines of disruption from the sidelines of disruption from the sidelines of disruption from the sidelines of disruption from the sidelines of disruption from the sidelines of disruption from the sidelines of disruption from the sidelines of disruption from the sidelines of disruption from the sidelines of disruption from the sidelines of disruption from the sidelines of disruption from the \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGnbQe1B9aGj",
        "outputId": "7914a1e3-82bf-48d1-9cec-7dcf42203764"
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(31,31 + EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print(f'Epoch {epoch+1} Batch {batch} Loss {batch_loss.numpy():.4f}')\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix=checkpoint_prefix)\n",
        "\n",
        "  print(f'Epoch {epoch+1} Loss {total_loss/steps_per_epoch:.4f}')\n",
        "  print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 32 Batch 0 Loss 0.2969\n",
            "Epoch 32 Batch 100 Loss 0.3601\n",
            "Epoch 32 Batch 200 Loss 0.4054\n",
            "Epoch 32 Batch 300 Loss 0.3351\n",
            "Epoch 32 Loss 0.3620\n",
            "Time taken for 1 epoch 308.43 sec\n",
            "\n",
            "Epoch 33 Batch 0 Loss 0.2686\n",
            "Epoch 33 Batch 100 Loss 0.3541\n",
            "Epoch 33 Batch 200 Loss 0.2787\n",
            "Epoch 33 Batch 300 Loss 0.3302\n",
            "Epoch 33 Loss 0.3423\n",
            "Time taken for 1 epoch 307.38 sec\n",
            "\n",
            "Epoch 34 Batch 0 Loss 0.2981\n",
            "Epoch 34 Batch 100 Loss 0.3281\n",
            "Epoch 34 Batch 200 Loss 0.3430\n",
            "Epoch 34 Batch 300 Loss 0.4020\n",
            "Epoch 34 Loss 0.3246\n",
            "Time taken for 1 epoch 307.85 sec\n",
            "\n",
            "Epoch 35 Batch 0 Loss 0.3143\n",
            "Epoch 35 Batch 100 Loss 0.2744\n",
            "Epoch 35 Batch 200 Loss 0.3269\n",
            "Epoch 35 Batch 300 Loss 0.3269\n",
            "Epoch 35 Loss 0.3086\n",
            "Time taken for 1 epoch 307.39 sec\n",
            "\n",
            "Epoch 36 Batch 0 Loss 0.2763\n",
            "Epoch 36 Batch 100 Loss 0.2725\n",
            "Epoch 36 Batch 200 Loss 0.2816\n",
            "Epoch 36 Batch 300 Loss 0.3009\n",
            "Epoch 36 Loss 0.2944\n",
            "Time taken for 1 epoch 307.80 sec\n",
            "\n",
            "Epoch 37 Batch 0 Loss 0.2438\n",
            "Epoch 37 Batch 100 Loss 0.2802\n",
            "Epoch 37 Batch 200 Loss 0.2659\n",
            "Epoch 37 Batch 300 Loss 0.2827\n",
            "Epoch 37 Loss 0.2815\n",
            "Time taken for 1 epoch 307.79 sec\n",
            "\n",
            "Epoch 38 Batch 0 Loss 0.2781\n",
            "Epoch 38 Batch 100 Loss 0.2781\n",
            "Epoch 38 Batch 200 Loss 0.3108\n",
            "Epoch 38 Batch 300 Loss 0.3001\n",
            "Epoch 38 Loss 0.2679\n",
            "Time taken for 1 epoch 308.00 sec\n",
            "\n",
            "Epoch 39 Batch 0 Loss 0.2668\n",
            "Epoch 39 Batch 100 Loss 0.2311\n",
            "Epoch 39 Batch 200 Loss 0.2306\n",
            "Epoch 39 Batch 300 Loss 0.2870\n",
            "Epoch 39 Loss 0.2541\n",
            "Time taken for 1 epoch 307.66 sec\n",
            "\n",
            "Epoch 40 Batch 0 Loss 0.2179\n",
            "Epoch 40 Batch 100 Loss 0.2266\n",
            "Epoch 40 Batch 200 Loss 0.2341\n",
            "Epoch 40 Batch 300 Loss 0.2401\n",
            "Epoch 40 Loss 0.2421\n",
            "Time taken for 1 epoch 307.78 sec\n",
            "\n",
            "Epoch 41 Batch 0 Loss 0.2040\n",
            "Epoch 41 Batch 100 Loss 0.1980\n",
            "Epoch 41 Batch 200 Loss 0.2095\n",
            "Epoch 41 Batch 300 Loss 0.2472\n",
            "Epoch 41 Loss 0.2300\n",
            "Time taken for 1 epoch 307.62 sec\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gB-m3hNk9dnw",
        "outputId": "4e390e51-ef19-4cf9-91b1-bc56f6fbc082"
      },
      "source": [
        "translate(\"不行啊\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: <start> 不行 啊 <end>\n",
            "Predicted translation: but a lesson of hyper uncertainty <end> \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5oWhVxbKlYP",
        "outputId": "30fbd9c3-c1d0-402a-f843-6efe3a72d7a5"
      },
      "source": [
        "translate(\"相反，欧洲决策者拒绝采取货币刺激而实施了财政紧缩，无视其银行压力的加剧。\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: <start> 相反 欧洲 决策者 拒绝 采取 货币 刺激 而 实施 了 财政 紧缩 无视 其 银行 压力 的 加剧 <end>\n",
            "Predicted translation: instead of european governments can only with their stimulus measures . <end> \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "laCxNX3cKvus",
        "outputId": "1688028b-aefa-458f-eb8b-d4dd84d605dd"
      },
      "source": [
        "translate(\"简言之，2015年拖累全球经济的因素在新的一年里还会持续——有的甚至还会加剧。\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: <start> 简言之 年 拖累 全球 经济 的 因素 在 新 的 一年 里 还 会 持续 有 的 甚至 还会 加剧 <end>\n",
            "Predicted translation: the single gdp in the global economy ever rise in the next few . <end> \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFzazBDyK1OL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}